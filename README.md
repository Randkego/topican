# README.md

Topican provides topic analysis for Python3.

The current version provides only topican.print_words_associated_with_common_noun_groups.

This assumes topics can be identified from Nouns and a surrounding "context" word:
- spaCy is used to identify Nouns (and Proper nouns) in the text;
- nltk WordNet and spaCy together are used to group these nouns (first using WordNet hyponyms and then
  spaCy similarity if the noun is not found in WordNet)
- the top context words for each noun are then output for each noun group

Citations:
(1) Uses spaCy - a free, open-source package for "Industrial Strength NLP" (Natural Language Processing)
(2) spaCy uses gloVe - Global Vectors for Word Representation Ref: Jeffrey Pennington, Richard Socher, and
Christopher D. Manning. 2014. https://nlp.stanford.edu/pubs/glove.pdf

![](header.png)

## Installation

Pre-requisites:

```sh
# Required packages
pip3 install pandas
pip3 install nltk
pip3 install spacy

# To use the large spaCy English language model
# Warning: this requires approx 1GB disk space
python3 -m spacy download en_core_web_lg
```

Topican:

```sh
pip3 install topican
```

## Usage example

topican.print_words_associated_with_common_noun_groups(
    nlp, name, free_text_Series, exclude_words, top_n_noun_groups, top_n_words, max_hyponyms, max_hyponym_depth, sim_threshold)

Where
- name: descriptive name for free_text_Series
- free_text_Series: pandas Series of free_text in which to find the noun groups and associated words
- exclude_words: to ignore certain words, e.g. not so useful 'stop words' or artificial words.
  This should take one of the following values:
  - True: to ignore NTLK stop-words
  - A list of words to exclude
  - False or None otherwise
- top_n_noun_groups: number of noun groups to find (specify 'None' means find all noun/'synonym' groups)
- top_n_words: number of words that are associated with each noun group (specify 'None' for all words)
- max_hyponyms: the maximum number of hyponyms a word may have before it is ignored (this is used to
  exclude very general words that may not convey useful information)
- max_hyponym_depth: the the level of hyponym to extract ('None' means find deepest)
- sim_threshold: the spaCy similarity level that words must reach to qualify as being a 'synonym'

## Usage example

```python3

# Some text to test
import pandas as pd
df = pd.DataFrame({'Text_col' : ["I love Python", "I like python", "python", "I like C++", "I don't like C++ any more"]})

# Load spaCy's large English language model (the large model is required to be able to use similarity)
# Warning: this requires approx 2GB of RAM
import spacy
nlp = spacy.load('en_core_web_lg')

import topican
topican.print_words_associated_with_common_noun_groups(nlp, "test", df['Text_col'], True, 10, None, 100, 1, 0.7)
```
![](usage_output.png)

## Development setup

Describe how to install all development dependencies and how to run an automated test-suite of some kind. Potentially do this for multiple platforms.

```sh
make install
npm test
```

## Release History

* 0.0.15
    * Work in progress

## Meta

Richard Smith â€“ randkego@gmail.com

Distributed under the MIT license. See ``LICENSE`` for more information.

[https://github.com/yourname/github-link](https://github.com/dbader/)

## Contributing

1. Fork it (<https://github.com/yourname/yourproject/fork>)
2. Create your feature branch (`git checkout -b feature/fooBar`)
3. Commit your changes (`git commit -am 'Add some fooBar'`)
4. Push to the branch (`git push origin feature/fooBar`)
5. Create a new Pull Request

<!-- Markdown link & img dfn's -->
[wiki]: https://github.com/yourname/yourproject/wiki
