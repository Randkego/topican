{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Analyser\n",
    "\n",
    "Output topics contained within free-text.\n",
    "\n",
    "\n",
    "## Citations:\n",
    "\n",
    "(1) Uses spaCy - a free, open-source package for \"Industrial Strength NLP\" (Natural Language Processing)\n",
    "\n",
    "(2) spaCy uses gloVe - Global Vectors for Word Representation Ref: Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. https://nlp.stanford.edu/pubs/glove.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sample reasons and all reasons Dataframes from CSV files\n",
    "import pandas as pd\n",
    "import os as os\n",
    "\n",
    "project_fileloc = './'\n",
    "sample_reasons_filename = 'Data_Final_N=14,645_6_17_18_sample_reasons.csv'\n",
    "sample_reasons_filepath = project_fileloc + sample_reasons_filename\n",
    "if os.path.exists(sample_reasons_filepath):\n",
    "    sample_reasons = pd.read_csv(sample_reasons_filepath)\n",
    "else:\n",
    "    print(\"File\", sample_reasons_filepath, \"not found\")\n",
    "    \n",
    "all_reasons_filename = 'Data_Final_N=14,645_6_17_18_all_reasons.csv'\n",
    "all_reasons_filepath = project_fileloc + all_reasons_filename\n",
    "if os.path.exists(all_reasons_filepath):\n",
    "    all_reasons = pd.read_csv(all_reasons_filepath)\n",
    "else:\n",
    "    print(\"File\", all_reasons_filepath, \"not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install spaCy** - \"Industrial strength NLP\" - to use its Natural Language Processing tools and its gloVe word vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#??? un-comment if not already done\n",
    "!pip install spacy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#??? if not already done, download all the spaCy model for English - use the large model to get the vectors\n",
    "!python -m spacy download  en_core_web_lg\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_MODEL = 'en_core_web_lg'   # make sure to use larger model for the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "print(\"*** WARNING: spaCy model\", SPACY_MODEL, \"takes approx. 2GB of memory\")\n",
    "nlp = spacy.load(SPACY_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_RESPONSE_STR = 'missing_no_answer_given'\n",
    "NOT_YET_CODED_STR = 'missing_not_yet_coded'\n",
    "NOT_APPLICABLE_STR = 'not_applicable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial words/word-phrases to ignore - as requested by the researchers\n",
    "exclude_words=['junk', NO_RESPONSE_STR, NOT_YET_CODED_STR, NOT_APPLICABLE_STR]\n",
    "capitalized = [w.capitalize() for w in exclude_words]\n",
    "for word in capitalized: exclude_words.append(word)\n",
    "exclude_words_without_stop_words = exclude_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'stop' words and their capitalisations to the list of words to ignore\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "for word in stop_words: exclude_words.append(word)\n",
    "capitalized_stop_words = [w.capitalize() for w in stop_words]\n",
    "for word in capitalized_stop_words: exclude_words.append(word)\n",
    "\n",
    "print(\"words that will be excluded (not so useful 'stop-words' and artificial words):\", exclude_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "Package: Topican - Topic Analyser\n",
    "Module: topican_by_nouns.py\n",
    "Identify topics by finding the top noun groups (nouns with 'synonyms') in free-text pandas and the top words\n",
    "words associated with them.\n",
    " \n",
    "Citations:\n",
    "(1) Uses spaCy - a free, open-source package for \"Industrial Strength NLP\" (Natural Language Processing)\n",
    "(2) spaCy uses gloVe - Global Vectors for Word Representation Ref: Jeffrey Pennington, Richard Socher, and\n",
    "Christopher D. Manning. 2014. https://nlp.stanford.edu/pubs/glove.pdf\n",
    "\"\"\"\n",
    "\n",
    "# Name for group containings words that are not known in the language model (potentially spelling errors)\n",
    "_UNKNOWN_GROUP_ROOT_WORD = \"_Unknown/Spelling_Error\"\n",
    "\n",
    "# Name for group containings words that are not known in the language model but were not grouped\n",
    "_OTHER_GROUP_ROOT_WORD = \"_OTHER\"\n",
    "\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import WordNetError\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "class SpaCyFreeText():\n",
    "    \"\"\"\n",
    "    Class to perform some spaCy operations on a free-text Panda Series object.\n",
    "    \n",
    "    spaCy has many powerful features, including Part-Of-Speech tagging (POS) and Dependency Trees to yield a deeper\n",
    "    understanding of text. \n",
    "\n",
    "    Any NaN values in the text are replaced by the empty string.\n",
    "    To aid Part Of Speech and dependency tree parsing, a full-stop is added to any items in the Series\n",
    "    that do not already end in punctuation.\n",
    "    \n",
    "    As the size of the text data is not very small, spaCyâ€™s `pipe` is used to iterate over the text. This \n",
    "    improves speed by accumulating a buffer and operating on the text in parallel (NB the argument\n",
    "    'n_threads' appears to make no difference).\n",
    "\n",
    "    Warning: At the time of coding, spaCy token.similarity(other) has a bug when 'other' has length 1, \n",
    "    producing the following error:\n",
    "        \"TypeError: 'spacy.tokens.token.Token' object does not support indexing\"\n",
    "        \n",
    "    The work-around used is to ignore tokens with length 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nlp, name, free_text_Series):\n",
    "        \"\"\"\n",
    "        Instantiate the class with the specified spaCy nlp object.\n",
    "        \"\"\"\n",
    "        self.nlp = nlp\n",
    "        self.name = name\n",
    "        \n",
    "        self.free_text_list = []\n",
    "        for value in free_text_Series.fillna(\"\"):\n",
    "            if len(value) > 0:\n",
    "                if value[-1] not in string.punctuation: value += \".\"\n",
    "            self.free_text_list.append(value)\n",
    "        free_text_Series = pd.Series(self.free_text_list)\n",
    "\n",
    "        # Create a spaCy document for each item in the free-text\n",
    "        self.doc_list = []\n",
    "        for doc in self.nlp.pipe(free_text_Series):\n",
    "            self.doc_list.append(doc)\n",
    "\n",
    "    def get_name(self): return self.name\n",
    "\n",
    "    def get_free_text_list(self): return self.free_text_list\n",
    "    \n",
    "    def get_doc_list(self): return self.doc_list\n",
    "\n",
    "    def get_most_common_pos(self, pos, top_n = 10, exclude_words=[]):\n",
    "        \"\"\"\n",
    "        Get top_n most common Parts Of Speech (top n nouns, verbs etc.)\n",
    "        Specify pos as 'None' to match all parts-of-speech\n",
    "        Specify top_n as 'None' to print all words\n",
    "        Use exclude_words to ignore certain words, e.g. not so useful 'stop words' or artificial words.\n",
    "        \"\"\"\n",
    "        word_list = []\n",
    "        for doc in self.doc_list:\n",
    "            for token in doc:\n",
    "                # Allow for matching multiple POS, e.g. \"NOUN,PROPN\" or all POS if pos=None\n",
    "                if not pos or token.pos_ in pos.split(\",\"):\n",
    "                    token_no_end_punc = token.lower_.rstrip(string.punctuation)\n",
    "                    if token_no_end_punc not in exclude_words:\n",
    "                        word_list.append(token_no_end_punc)\n",
    "                    # ??? TODO: make use of lower_ and rstrip configurable?\n",
    "        most_common_pos = Counter(word_list).most_common(top_n)\n",
    "        return(most_common_pos)\n",
    "\n",
    "    def print_most_common_pos(self, pos, top_n = 10, exclude_words=[]):\n",
    "        if top_n:\n",
    "            print(\"Top\", top_n, end='')\n",
    "        else:\n",
    "            print(\"All\", end='')\n",
    "        print(\" \" + pos + \"s for\", self.name)\n",
    "        most_common_pos = self.get_most_common_pos(pos, top_n, exclude_words)\n",
    "        print(most_common_pos)\n",
    "        \n",
    "    def print_most_common_pos_with_words_before(self, pos, top_n = 10, exclude_words=[]):\n",
    "        \"\"\"\n",
    "        Print top_n words with the specified part-of-speech and their preceeding word (if any).\n",
    "        Specify pos as 'None' to match all parts-of-speech\n",
    "        Specify top_n as 'None' to print all words\n",
    "        Use exclude_words to ignore certain words, e.g. not so useful 'stop words' or artificial words.\n",
    "        \"\"\"\n",
    "        NO_WORD_BEFORE = \"no_word_before\" \n",
    "        \n",
    "        ## For debugging, create a fixed-size buffer to hold the top_n words_before\n",
    "        ##import collections\n",
    "        ##debug_prev_words_deque = collections.deque(maxlen=top_n)\n",
    "        ##for i in range(top_n): debug_prev_words_deque.append(NO_WORD_BEFORE)\n",
    "        \n",
    "        word_list = [] # List of words used to determine the top_n\n",
    "        word_dict = {} # Dictionary of words used to hold their preceding words\n",
    "        token_before = \"\"\n",
    "        for doc in self.doc_list:\n",
    "            for token in doc:\n",
    "                token_no_end_punc = token.lower_.rstrip(string.punctuation)\n",
    "                \n",
    "                # Allow for matching multiple POS, e.g. \"NOUN,PROPN\" or all POS if pos=None\n",
    "                if not pos or token.pos_ in pos.split(\",\"):\n",
    "                    if token_no_end_punc not in exclude_words:\n",
    "                        word_list.append(token_no_end_punc)\n",
    "                        ##if token_no_end_punc == \"<some anomaly>\": print(\"DEBUG: '<some anomaly>' was preceded by: \", debug_prev_words_deque)\n",
    "\n",
    "                        if token_no_end_punc not in word_dict:\n",
    "                            word_dict[token_no_end_punc] = []\n",
    "\n",
    "                        if not token_before or token_before in string.punctuation:\n",
    "                            token_before = NO_WORD_BEFORE\n",
    "\n",
    "                        # Add the word before the specified part-of-speech word to the list of words found before it\n",
    "                        word_dict[token_no_end_punc].append(token_before)\n",
    "                        \n",
    "                token_before = token_no_end_punc\n",
    "                ##debug_prev_words_deque.append(token_before)\n",
    "        most_common_words = Counter(word_list).most_common(top_n)\n",
    "        if top_n:\n",
    "            print(\"Top\", top_n, end='')\n",
    "        else:\n",
    "            print(\"All\", end='')\n",
    "        print(pos + \"s and top preceeding words for\", self.name)\n",
    "        for word, word_count in most_common_words:\n",
    "            print(\"['\" + word + \"', \" + str(word_count) + \"]\", \"has top preceding words (freq>1): \", end=(''))\n",
    "            \n",
    "            most_common_words_before = Counter(word_dict[word]).most_common(top_n)\n",
    "            for word_before, word_before_count in most_common_words_before:\n",
    "                if word_before_count > 1:\n",
    "                    print(\"('\" + word_before + \"', \" + str(word_before_count) + \") \", end=(''))\n",
    "            print()\n",
    "        \n",
    "    def get_most_common_nouns(self, top_n = 10, exclude_words=[]):\n",
    "        return self.get_most_common_pos(\"NOUN\", top_n, exclude_words)\n",
    "\n",
    "    def print_most_common_nouns(self, top_n = 10, exclude_words=[]):\n",
    "        self.print_most_common_pos(\"NOUN\", top_n, exclude_words)\n",
    "\n",
    "    def get_most_common_nouns_and_propns(self, top_n = 10, exclude_words=[]):\n",
    "        return self.get_most_common_pos(\"NOUN,PROPN\", top_n, exclude_words)\n",
    "\n",
    "    def print_most_common_nouns_and_propns(self, top_n = 10, exclude_words=[]):\n",
    "        self.print_most_common_pos(\"NOUN,PROPN\", top_n, exclude_words)\n",
    "\n",
    "    def get_most_common_adjs(self, top_n = 10, exclude_words=[]):\n",
    "        return self.print_most_common_pos(\"ADJ\", top_n, exclude_words)\n",
    "        \n",
    "    def print_most_common_adjs(self, top_n = 10, exclude_words=[]):\n",
    "        self.print_most_common_pos(\"ADJ\", top_n, exclude_words)\n",
    "\n",
    "    def get_most_common_verbs(self, top_n = 10, exclude_words=[]):\n",
    "        return self.print_most_common_pos(\"VERB\", top_n, exclude_words)\n",
    "\n",
    "    def print_most_common_verbs(self, top_n = 10, exclude_words=[]):\n",
    "        self.print_most_common_pos(\"VERB\", top_n, exclude_words)\n",
    "        \n",
    "    def get_most_common_adverbs(self, top_n = 10, exclude_words=[]):\n",
    "        return self.print_most_common_pos(\"ADV\", top_n, exclude_words)\n",
    "        \n",
    "    def print_most_common_adverbs(self, top_n = 10, exclude_words=[]):\n",
    "        self.print_most_common_pos(\"ADV\", top_n, exclude_words)\n",
    "        \n",
    "    def print_most_common_noun_chunks(self, top_n = 10, exclude_list=[]):\n",
    "        \"\"\"\n",
    "        Print top_n noun chunks (noun phrases) - potentially giving more context to nouns;\n",
    "        specify 'None' for all.\n",
    "        ??? TODO: ** possibly make lower_ and rstrip configurable?\n",
    "        \"\"\"\n",
    "        noun_chunk_list = []\n",
    "        for doc in self.doc_list:\n",
    "            for chunk in doc.noun_chunks:\n",
    "                noun_chunk_str = ''\n",
    "                if exclude_list == None:\n",
    "                    noun_chunk_str = chunk.lower_.rstrip(string.punctuation)\n",
    "                else:\n",
    "                    for w in chunk.lower_.split():\n",
    "                        w_no_end_punc = w.rstrip(string.punctuation)\n",
    "                        if w_no_end_punc not in exclude_list:\n",
    "                            noun_chunk_str += w_no_end_punc + ' '\n",
    "                noun_chunk_str = noun_chunk_str.strip()\n",
    "                if len(noun_chunk_str) > 0:\n",
    "                    noun_chunk_list.append(noun_chunk_str)\n",
    "        most_common_noun_chunks = Counter(noun_chunk_list).most_common(top_n)\n",
    "        print(most_common_noun_chunks)\n",
    "        \n",
    "    def print_most_common_dep_trees(self, top_n = 10, exclude_list=[]):\n",
    "        \"\"\"\n",
    "        Print top_n sentence dependency trees - potentially giving more context to text.\n",
    "        top_n is the number of items to print; specify 'None' for all\n",
    "        TODO: tidy the output for more meaning.\n",
    "        \"\"\"\n",
    "\n",
    "        if exclude_list != None:\n",
    "            print(\"** exclude_list not implemented in order to preserve sentence structure\")\n",
    "        dep_tree_list = []\n",
    "        for doc in self.doc_list:\n",
    "            for token in doc:\n",
    "                if token.lower_ not in string.punctuation:\n",
    "                    dep_tree_str = \" \".join(\n",
    "                        [token.text, token.dep_, token.head.text, token.head.pos_, \"[\"])\n",
    "                    child_list = []\n",
    "                    for child in token.children:\n",
    "                        if child.lower_ not in string.punctuation:\n",
    "                            child_list.append(child.lower_.rstrip(string.punctuation))\n",
    "                    dep_tree_str += ','.join(child_list) + \"]\"\n",
    "                    dep_tree_list.append(dep_tree_str)\n",
    "        most_common_dep_trees = Counter(dep_tree_list).most_common(top_n)\n",
    "        if top_n:\n",
    "            print(\"Top\", top_n, end='')\n",
    "        else:\n",
    "            print(\"All\", end='')\n",
    "        print(\" dep trees for\", self.name, most_common_dep_trees)\n",
    "\n",
    "def make_synset(word, category='n', number='01'):\n",
    "    \"\"\"Conveniently make a synset\"\"\"\n",
    "    number = int(number)\n",
    "    synset = wn.synset('%s.%s.%02i' % (word, category, number))\n",
    "    return synset\n",
    "    \n",
    "def _recurse_all_hypernyms(synset, all_hypernyms):\n",
    "    synset_hypernyms = synset.hypernyms()\n",
    "    if synset_hypernyms:\n",
    "        all_hypernyms += synset_hypernyms\n",
    "        for hypernym in synset_hypernyms:\n",
    "            _recurse_all_hypernyms(hypernym, all_hypernyms)\n",
    "\n",
    "def all_hypernyms(synset):\n",
    "    \"\"\"Get the set of hypernyms of the hypernym of the synset etc.\n",
    "       Nouns can have multiple hypernyms, so we can't just create a depth-sorted\n",
    "       list.\"\"\"\n",
    "    hypernyms = []\n",
    "    _recurse_all_hypernyms(synset, hypernyms)\n",
    "    return set(hypernyms)\n",
    "\n",
    "def _recurse_all_hyponyms(synset, all_hyponyms, max_depth=None):\n",
    "    synset_hyponyms = synset.hyponyms()\n",
    "    if synset_hyponyms:\n",
    "        all_hyponyms += synset_hyponyms\n",
    "        if max_depth:\n",
    "            for level, hyponym in enumerate(synset_hyponyms):\n",
    "                if (level < max_depth-1):\n",
    "                    _recurse_all_hyponyms(hyponym, all_hyponyms)\n",
    "                else: break\n",
    "        else:\n",
    "            for hyponym in synset_hyponyms:\n",
    "                _recurse_all_hyponyms(hyponym, all_hyponyms)\n",
    "            \n",
    "def all_hyponyms(synset, max_depth=None):\n",
    "    \"\"\"Get the set of the tree of hyponyms under the synset\"\"\"\n",
    "    hyponyms = []\n",
    "    _recurse_all_hyponyms(synset, hyponyms, max_depth)\n",
    "    return set(hyponyms)\n",
    "\n",
    "def all_peers(synset):\n",
    "    \"\"\"Get the set of all peers of the synset (including the synset).\n",
    "       If the synset has multiple hypernyms then the peers will be hyponyms of\n",
    "       multiple synsets.\"\"\"\n",
    "    hypernyms = synset.hypernyms()\n",
    "    peers = []\n",
    "    for hypernym in hypernyms:\n",
    "        peers += hypernym.hyponyms()\n",
    "    return set(peers)\n",
    "\n",
    "def synset_word(synset):\n",
    "    return synset.name().split('.')[0]\n",
    "\n",
    "def synsets_words(synsets):\n",
    "    \"\"\"Get the set of strings for the words represented by the synsets\"\"\"\n",
    "    return set([synset_word(synset) for synset in synsets])\n",
    "\n",
    "def get_hypernyms(word):\n",
    "    \"\"\"\n",
    "    Get hypernyms for the specified word.\n",
    "    \"\"\"\n",
    "    synset = make_synset(word)\n",
    "    if synset:\n",
    "        hypernyms = synsets_words(all_hypernyms(synset))\n",
    "    else:\n",
    "        hypernyms = None\n",
    "    return hypernyms\n",
    "\n",
    "def get_hyponyms_and_for_peers(word):\n",
    "    \"\"\"\n",
    "    Get hyponyms for the specified word and hyponyms of the word's peers.\n",
    "    Output appears much too general e.g. for such words as 'work'\n",
    "    \"\"\"\n",
    "    synset = make_synset(word)\n",
    "    if synset:\n",
    "        hyponyms = synsets_words(all_hyponyms(synset))\n",
    "        peers = synsets_words(all_peers(synset))\n",
    "        hyponyms_of_peers = set()\n",
    "        for s in all_peers(synset):\n",
    "            hyponyms_of_peers |= synsets_words(all_hyponyms(s))\n",
    "        hyponyms_and_for_peers = hyponyms | peers | hyponyms_of_peers\n",
    "    else:\n",
    "        hyponyms_and_for_peers = None\n",
    "    return hyponyms_and_for_peers\n",
    "\n",
    "def get_hyponyms(word, max_depth=None):\n",
    "    \"\"\"\n",
    "    Get hyponyms for the specified word.\n",
    "    Use max_depth to specify the level of hyponym to extract\n",
    "    \"\"\"\n",
    "    synset = make_synset(word)\n",
    "    if synset:\n",
    "        hyponyms = synsets_words(all_hyponyms(synset, max_depth))\n",
    "    else:\n",
    "        hyponyms = None\n",
    "    return hyponyms\n",
    "\n",
    "def get_top_word_groups_by_synset_then_similarity(\n",
    "    nlp, word_freqs, n_word_groups, max_hyponyms, max_hyponym_depth, sim_threshold, user_defined_groups):\n",
    "    \"\"\"\n",
    "    Function to print the top 'n' word \"synonym\" groups using WordNet synsets followed by\n",
    "    spaCy similarity:\n",
    "    - nlp: spaCy object pre-initialised with the required langauge model\n",
    "    - word_freqs: list of (word,count) tuples in (decreasing) order of frequency\n",
    "    - n_word_groups: number of word groups to find (specify 'None' means find all word/'synonym' groups)\n",
    "    - max_hyponyms: the maximum number of hyponyms a word may have before it is ignored (this is used to\n",
    "      exclude very general words that may not convey useful information)\n",
    "    - max_hyponym_depth specifies the level of hyponym to extract ('None' means find deepest)\n",
    "    - sim_threshold: the spaCy similarity level that words must reach to qualify as being a 'synonym'\n",
    "    - user_defined_groups: initial user-defined groupings. Note that \"synonyms\" for artificial words, such as\n",
    "      'love_it', that are not known in the language model (either WordNet or spaCy) can only be matched exactly (as\n",
    "      clearly no synonym can be found). An additional restriction, is that artifical words must be listed as root\n",
    "      words and may not have pre-set \"synonyms\". An alternative where the \"synonyms\" of each root word is searched \n",
    "      for equivalence (or even \"synonymity\") might be possible but this would come with performance cost.\n",
    "    Returns list of tuples:\n",
    "    - (root-word word frequency tuple, list of root and synonym word frequency tuples)\n",
    "    - (OTHER-word frequency tuple if any are not matched, list of other word frequency tuples)\n",
    "    - (UNKNOWN-word frequency tuple if any are not found in the language model, list of unknown word frequency tuples)\n",
    "\n",
    "    Some considerations:\n",
    "    - The use of WordNet synsets or spaCy similarity by themselves are as useful as required. This is for 2 reasons:\n",
    "      (1) Some free-text may contain both formal and informal language.\n",
    "      The advantage of using spaCY with language model 'en_core_web_lg' is that this model was trained on both formal and informal\n",
    "      language. However, the word vectors in this spaCy language model incorporate all senses of a word and each part of speech. \n",
    "    - Hence a combination of WordNet synsets and spaCy similarity is used. WordNet synsets are applied first to parse\n",
    "      formally defined words, then spaCy similarity is used to catch some other words. A possible further\n",
    "      refinement might be to take user-defined groupings.\n",
    "    - It is assumed that 'stop' words and certain artificial words have been removed if required *before* the\n",
    "      function call (very common words such as \"I\" that may not be very useful in the word grouping)\n",
    "    - The language model may contain country-specific, e.g. in the US 'pissed' as the meaning 'angry' but a different meaning in\n",
    "      UK English.\n",
    "\n",
    "    Note: spaCy internally uses gloVe word vectors - see citations above.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Word \"lemmatization\" is necessary as plurals are not stored in the WordNet synsets\n",
    "    Lem = WordNetLemmatizer()\n",
    "    \n",
    "    # Create a dictionary containing the spaCy token for each word\n",
    "    # - for later use with spaCy similarity\n",
    "    document = ''\n",
    "    for word_freq in word_freqs:\n",
    "        word, count = word_freq\n",
    "        document += word + ' '        \n",
    "    spacy_dict = {}\n",
    "    for spacy_token in nlp(document):\n",
    "        word = spacy_token.lower_\n",
    "        spacy_dict[word] = spacy_token\n",
    "    #for item in spacy_dict: print(\"DEBUG: spacy_dict[\" + item + \"]: \" + spacy_dict[item])\n",
    "    \n",
    "    # Build a list of most-common word \"groups\", grouping words that are WordNet \"hyponyms\"\n",
    "    most_common = OrderedDict()        # The most common words and their synonyms.\n",
    "                                       # Note: this needs to be in order of insertion in order to retain the\n",
    "                                       # frequency order from word_freqs (insertion order is only guaranteed in the\n",
    "                                       # default dictionaries from CPython 3.6+)\n",
    "    potential_spelling_errors = set()\n",
    "    already_grouped = set()            # Words already grouped - either a root word or one of its hyponyms\n",
    "    first_stored = False\n",
    "    \n",
    "    if user_defined_groups:\n",
    "        # Add the user-defined \"synonym\" groups\n",
    "        for _tuple in user_defined_groups:\n",
    "            root_word_tuple, root_and_syn_tuple_list = _tuple\n",
    "            root_word, root_and_syn_count = root_word_tuple\n",
    "            root_word = root_word.lower()\n",
    "            if root_word[0] == '_': root_word = root_word[1:]\n",
    "            ##print(\"DEBUG: Adding pre-defined root word '\" + root_word + \"' with root_and_syn_tuple_list:\",\n",
    "            ##      root_and_syn_tuple_list)\n",
    "            try:\n",
    "                lemword = Lem.lemmatize(root_word)\n",
    "                hyponyms = get_hyponyms(lemword, max_hyponym_depth)\n",
    "            except WordNetError:\n",
    "                # Word not known in the WordNet synsets but store it anyway as it's a pre-defined word\n",
    "                # but mark it as a potential spelling error\n",
    "                ##print(\"DEBUG: pre-defined word not known in the WordNet synsets: '\" + root_word + \"'\")\n",
    "                lemword = root_word\n",
    "                hyponyms = []\n",
    "                potential_spelling_errors.add(root_word)\n",
    "            most_common[root_word] = {}\n",
    "            most_common[root_word]['root_and_syn_count'] = 0\n",
    "            most_common[root_word]['root_and_syns'] = root_and_syn_tuple_list\n",
    "            most_common[root_word]['hyponyms'] = hyponyms\n",
    "            ##print(\"DEBUG: with hyponyms:\", hyponyms)\n",
    "            most_common[root_word]['lemmatization'] = lemword\n",
    "            ##print(\"DEBUG: and lemmatization:\", lemword)\n",
    "            try:\n",
    "                most_common[root_word]['root_token'] = spacy_dict[root_word]\n",
    "            except KeyError:\n",
    "                # No spaCy token found for the root token so create one\n",
    "                spaCy_doc_root_word = nlp(root_word)\n",
    "                spaCy_token_root_word = None\n",
    "                for item in spaCy_doc_root_word:\n",
    "                    spaCy_token_root_word = item                        \n",
    "                most_common[root_word]['root_token'] = spaCy_token_root_word\n",
    "            already_grouped.add(root_word)\n",
    "            first_stored = True\n",
    "            for word_tuple in root_and_syn_tuple_list:\n",
    "                word, word_count = word_tuple\n",
    "                ##print(\"DEBUG: with pre-defined synonym: '\" + word + \"'\")\n",
    "            \n",
    "    for word_freq in word_freqs:\n",
    "        word, count = word_freq\n",
    "        word = word.lower()\n",
    "        if word not in already_grouped:\n",
    "            ##print(\"DEBUG: word: '\" + word + \"', count: \" + str(count))\n",
    "            if (not first_stored):\n",
    "                try:\n",
    "                    lemword = Lem.lemmatize(word)\n",
    "                    hyponyms = get_hyponyms(lemword, max_hyponym_depth)\n",
    "                    if hyponyms and len(hyponyms) > max_hyponyms:\n",
    "                        continue\n",
    "                        ##print(\"DEBUG: Word '\" + word + \"'\" + \"(lemmatized as '\" + lemword + \"') exceeds hyponym limit\")\n",
    "                    else:\n",
    "                        # Store the first most common word as the 'root' of the first word group\n",
    "                        ##print(\"DEBUG: Adding root word '\" + word + \"' with hyponyms:\", hyponyms)\n",
    "                        most_common[word] = {}\n",
    "                        most_common[word]['root_and_syn_count'] = count\n",
    "                        most_common[word]['root_and_syns'] = [word_freq]\n",
    "                        most_common[word]['hyponyms'] = hyponyms\n",
    "                        most_common[word]['lemmatization'] = lemword\n",
    "                        most_common[word]['root_token'] = spacy_dict[word]\n",
    "                        already_grouped.add(word)\n",
    "                        first_stored = True\n",
    "                except WordNetError:\n",
    "                    ##print(\"DEBUG: Word not known in the WordNet synsets: '\" + word + \"'\")\n",
    "                    potential_spelling_errors.add(word)\n",
    "            else:\n",
    "                # Store the common word as a new word group if there are no existing \"synonyms\" in the most_common dictionary\n",
    "                synonym_or_match_found = False\n",
    "                for common in most_common:\n",
    "                    if common == word:\n",
    "                        # word is already a 'root' word group\n",
    "                        synonym_or_match_found = True\n",
    "                    else:\n",
    "                        if common not in potential_spelling_errors:\n",
    "                            if word not in already_grouped:\n",
    "                                lemword = Lem.lemmatize(word)\n",
    "                                if (lemword == most_common[common]['lemmatization'] or\n",
    "                                    lemword in most_common[common]['hyponyms']):\n",
    "                                    # synonym found - incorporate it under the 'root' synonym\n",
    "                                    synonym_or_match_found = True\n",
    "                                    most_common[common]['root_and_syn_count'] += count\n",
    "                                    most_common[common]['root_and_syns'].append(word_freq)\n",
    "                                    already_grouped.add(word)\n",
    "\n",
    "                if not synonym_or_match_found:\n",
    "                    try:\n",
    "                        lemword = Lem.lemmatize(word)\n",
    "                        hyponyms = get_hyponyms(lemword, max_hyponym_depth)\n",
    "                        if hyponyms and len(hyponyms) > max_hyponyms:\n",
    "                            continue\n",
    "                            ##print(\"DEBUG: Word '\" + word + \"'\" + \"(lemmatized as '\" + lemword + \"') exceeds hyponym limit\")\n",
    "                        else:\n",
    "                            ##print(\"DEBUG: Adding root word '\" + word + \"' with hyponyms:\", hyponyms)\n",
    "                            most_common[word] = {}\n",
    "                            most_common[word]['root_and_syn_count'] = count\n",
    "                            most_common[word]['root_and_syns'] = [word_freq]\n",
    "                            most_common[word]['hyponyms'] = hyponyms\n",
    "                            most_common[word]['lemmatization'] = lemword\n",
    "                            most_common[word]['root_token'] = spacy_dict[word]\n",
    "                            already_grouped.add(word)\n",
    "                    except WordNetError:\n",
    "                        ##print(\"DEBUG: Word not known in the WordNet synsets: '\" + word + \"'\")\n",
    "                        potential_spelling_errors.add(word)\n",
    "\n",
    "    # Create a spaCy token for a common word such as 'the' - for later use in detecting unknown words\n",
    "    spaCy_doc_the = nlp(\"the\")\n",
    "    spaCy_token_the = None\n",
    "    for item in spaCy_doc_the:\n",
    "        spaCy_token_the = item                        \n",
    "\n",
    "    # Now apply spaCy similarity to try to group words for which a hyponym was not found or\n",
    "    # which were not known in the WordNet synsets ...\n",
    "    other_words = set()\n",
    "    for word_freq in word_freqs:\n",
    "        word, count = word_freq\n",
    "        if word not in already_grouped:\n",
    "            other_words.add(word)\n",
    "            ##print(\"DEBUG: Adding '\" + word + \"' not in already_grouped to other_words\")\n",
    "    for word in potential_spelling_errors:\n",
    "        other_words.add(word)\n",
    "        ##print(\"DEBUG: Adding '\" + word + \"' in potential_spelling_errors to other_words\")\n",
    "\n",
    "    # ... i.e. update the list of most-common word \"groups\" formed from WordNet synsets with words that have similar\n",
    "    # enough spaCy token.similarity. Words that were not known in synsets may be known in spaCy so reset the list of\n",
    "    # potential spelling errors. Note that tokens with a length of 1 are ignored as a work-around for a spaCy\n",
    "    # token.similarity(other) bug\n",
    "\n",
    "    potential_spelling_errors = set()\n",
    "    already_grouped = set()\n",
    "    first_stored = False\n",
    "    for word_freq in word_freqs:\n",
    "        word, count = word_freq\n",
    "        if word in other_words:\n",
    "            ##print(\"DEBUG: word in other_words: '\" + word + \"', count: \" + str(count))\n",
    "            if word not in already_grouped:\n",
    "                ##print(\"DEBUG: word not in already_grouped: '\" + word + \"', count: \" + str(count))\n",
    "                if len(word) > 1: # Work-around for bug in spaCy if token has length of 1\n",
    "                    synonym_or_match_found = False\n",
    "                    best_match = None\n",
    "                    best_match_similarity = 0\n",
    "                    for common in most_common:\n",
    "                        if len(common) > 1:    # Work-around for bug in spaCy if token has length of 1\n",
    "                            common_token = most_common[common]['root_token']\n",
    "                            try:\n",
    "                                token = spacy_dict[word]\n",
    "                            except KeyError:\n",
    "                                # No spaCy token found for the word so create one\n",
    "                                spaCy_doc_word = nlp(word)\n",
    "                                spaCy_token_word = None\n",
    "                                for item in spaCy_doc_word:\n",
    "                                    spaCy_token_word = item                        \n",
    "                                    token = spaCy_token_word\n",
    "                            ##print(\"DEBUG: Similarity between token '\", token, \"' and common item '\",\n",
    "                            ##      common, \"' is:\", token.similarity(common_token))\n",
    "                            if (common_token.lower_ == token.lower_):\n",
    "                                # word is already a 'root' word group\n",
    "                                synonym_or_match_found = True\n",
    "                                break\n",
    "                            else:\n",
    "                                # check \"word\" is not a potential spelling error/unknown to spaCy\n",
    "                                if common_token.lower_ not in potential_spelling_errors:\n",
    "                                    similarity = token.similarity(common_token)\n",
    "                                    if similarity >= sim_threshold:\n",
    "                                        if similarity > best_match_similarity:\n",
    "                                            best_match = common\n",
    "                                            best_match_similarity = similarity\n",
    "                                    elif similarity == 0:\n",
    "                                        if word not in potential_spelling_errors:\n",
    "                                            # Check similarity of word against a known word such as'the' in case\n",
    "                                            # common_token is a word that is known by WordNet but not by spaCy\n",
    "                                            if token.similarity(spaCy_token_the) == 0:\n",
    "                                                ##print(\"DEBUG: Adding potential spelling error for '\" + word,\n",
    "                                                ##      \"' - common_token was '\" + common_token.lower_ +\n",
    "                                                ##      \"' similarity was \" + str(similarity))\n",
    "                                                potential_spelling_errors.add(word)\n",
    "                                                break\n",
    "                    if best_match:\n",
    "                        # incorporate the word under the 'root' synonym with the best similarity match\n",
    "                        synonym_or_match_found = True\n",
    "                        most_common[best_match]['root_and_syn_count'] += count\n",
    "                        most_common[best_match]['root_and_syns'].append(word_freq)\n",
    "                        already_grouped.add(word)\n",
    "                        \n",
    "    top_word_groups = []\n",
    "    other_words = []\n",
    "    for index, word in enumerate(most_common):\n",
    "        ##print(\"DEBUG: most_common: index \" + str(index) + \" word: '\" + word + \"'\")\n",
    "        if index < n_word_groups:\n",
    "            word_group_tuple = (\"_\" + word, most_common[word]['root_and_syn_count'])\n",
    "            root_and_syns_list = []\n",
    "            for word_freq in most_common[word]['root_and_syns']:\n",
    "                root_and_syns_list.append(word_freq)\n",
    "            top_word_groups.append((word_group_tuple, root_and_syns_list))\n",
    "        elif word not in potential_spelling_errors:\n",
    "            other_words.append(word)\n",
    "    \n",
    "    if other_words:\n",
    "        # Words that were not potential spelling errors that did not make it into a word group\n",
    "        total_other_words = 0\n",
    "        other_word_freqs = []\n",
    "        for word in other_words:\n",
    "            word_count = 0\n",
    "            for word_freq in word_freqs:\n",
    "                w, count = word_freq\n",
    "                if w == word:\n",
    "                    word_count = count\n",
    "                    other_word_freqs.append(word_freq)\n",
    "                    break;\n",
    "            total_other_words += word_count\n",
    "        other_word_group_tuple = (_OTHER_GROUP_ROOT_WORD, total_other_words)\n",
    "        top_word_groups.append((other_word_group_tuple, other_word_freqs))\n",
    "        \n",
    "    if len(potential_spelling_errors) > 0:\n",
    "        # Potential spelling errors / words not in the spaCy model that (may) have been ignored\n",
    "        total_unknown_words = 0\n",
    "        unknown_word_freqs = []\n",
    "        for word in potential_spelling_errors:\n",
    "            word_count = 0\n",
    "            for word_freq in word_freqs:\n",
    "                w, count = word_freq\n",
    "                if w == word:\n",
    "                    word_count = count\n",
    "                    unknown_word_freqs.append(word_freq)\n",
    "                    break;\n",
    "            total_unknown_words += word_count\n",
    "        unknown_word_group_tuple = (_UNKNOWN_GROUP_ROOT_WORD, total_unknown_words)\n",
    "        top_word_groups.append((unknown_word_group_tuple, unknown_word_freqs))\n",
    "    \n",
    "    # Check all words were accounted for\n",
    "    for word in already_grouped:\n",
    "        word_found = False\n",
    "        for word_group in most_common:\n",
    "            for word_freq in most_common[word_group]['root_and_syns']:\n",
    "                w, count = word_freq\n",
    "                if word == w: word_found = True\n",
    "        if not word_found: print(\"*** Error: \" + word + \" not found in most_common\")\n",
    "    \n",
    "    return top_word_groups\n",
    "\n",
    "\n",
    "# Print topics by finding words associated with each noun group\n",
    "def get_words_around(target, text, n_words):\n",
    "    \"\"\"\n",
    "    Helper function for print_words_associated_with_common_noun_groups to return n_words before or\n",
    "    after each occurence of a word in some text.\n",
    "    The function returns a list (or lists) of the n_words before and after the 'target' word.\n",
    "    \"\"\"\n",
    "    words = text.lower().rstrip(string.punctuation).split()\n",
    "    for i,w in enumerate(words):\n",
    "        if w == target:\n",
    "            if (i<n_words): yield words[0:i], words[i+1:i+1+n_words]\n",
    "            else: yield words[i-n_words:i], words[i+1:i+1+n_words]\n",
    "    \n",
    "def print_words_associated_with_common_noun_groups(\n",
    "    nlp, name, free_text_Series, exclude_words, top_n_noun_groups, top_n_words,\n",
    "    max_hyponyms, max_hyponym_depth, sim_threshold):\n",
    "    \"\"\"\n",
    "    Find the top noun groups (nouns with 'synonyms') in free_text_Series and print the top_n_words associated\n",
    "    with them:\n",
    "    - nlp: spaCy object pre-initialised with the required langauge model\n",
    "    - name: descriptive name for free_text_Series\n",
    "    - free_text_Series: pandas Series of free_text in which to find the noun groups and associated words\n",
    "    - exclude_words: to ignore certain words, e.g. not so useful 'stop words' or artificial words.\n",
    "      This should take one of the following values:\n",
    "      - True: to ignore NTLK stop-words\n",
    "      - A list of words to exclude\n",
    "      - False or None otherwise\n",
    "    - top_n_noun_groups: number of noun groups to find (specify 'None' means find all noun/'synonym' groups)\n",
    "    - top_n_words: number of words that are associated with each noun group (specify 'None' for all words)\n",
    "    - max_hyponyms: the maximum number of hyponyms a word may have before it is ignored (this is used to\n",
    "      exclude very general words that may not convey useful information)\n",
    "    - max_hyponym_depth: the the level of hyponym to extract ('None' means find deepest)\n",
    "    - sim_threshold: the spaCy similarity level that words must reach to qualify as being a 'synonym'\n",
    "    Notes:\n",
    "    (1) For best results, stop words should be excluded\n",
    "    (2) If stop words are excluded, negations such as \"no\", \"not\" and words ending with \"n't\" are still considered\n",
    "        in the parsing (within the limits of the word context length NUM_CONTEXT_WORDS)\n",
    "    (3) If no associated word is found, it is assumed the word itself is the only context for the text. For example,\n",
    "        with noun group \"_work\", the following free text items would result in the noun itself ('work') being\n",
    "        reported as an associated word: 'The work that I do', 'Work', 'work.'\n",
    "        This seems to work in the majority of cases but has not been exhaustively tested. In\n",
    "        particular, this might give surprising results if the \"synonym\" matching does not actually give the desired\n",
    "        synonyms and may give incorrect results if NUM_CONTEXT_WORDS is too short to capture significant context.\n",
    "        \n",
    "    Known restrictions/issues:\n",
    "    (a) The parsing of associated words looks only at words that are up to NUM_CONTEXT_WORDS words before or after\n",
    "        each noun, inclusive of any exclude words. This limit could be parameterised in this function (it already is\n",
    "        in helper function 'get_words_around')\n",
    "    (b) The context words before the noun are parsed first since in the samples tested it appears the \"before-words\"\n",
    "        usually give more context than words proceeding a nound. Clearly. this may give sub-optimal results for text\n",
    "        where the main context for the noun comes in the words following it.\n",
    "    (c) The parsing of hyphenated words may not always work correctly.\n",
    "    (d) Spelling mistakes or grammatical errors in the original text may give surprising results.\n",
    "    (e) See also notes in helper function get_top_word_groups_by_synset_then_similarity and class SpaCyFreeText\n",
    "    (f) Significantly more testing is required to verify usefulness and robusteness.\n",
    "\n",
    "    Possible enhancements:\n",
    "    (i)   NUM_CONTEXT_WORDS could be parameterised, as described above\n",
    "    (ii)  The output of associated words could be limited to those having a frequency>1\n",
    "    (iii) Words associated with each noun could be additionally grouped in \"valence\" sub-groups\n",
    "    (iv)  For large text, parallelisation would help performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    text_spaCy = SpaCyFreeText(nlp, name, free_text_Series)\n",
    "    free_text_list = text_spaCy.get_free_text_list()\n",
    "    \n",
    "    exclude_word_list = []\n",
    "    if exclude_words==True:\n",
    "        # Exclude 'stop' words and their capitalisations\n",
    "        stop_words = stopwords.words('english')\n",
    "        for word in stop_words: exclude_word_list.append(word)\n",
    "        capitalized_stop_words = [w.capitalize() for w in stop_words]\n",
    "        for word in capitalized_stop_words: exclude_word_list.append(word)\n",
    "    elif exclude_words:\n",
    "        exclude_word_list = exclude_words\n",
    "    if exclude_words:\n",
    "        print(\"Words that will be excluded:\", exclude_word_list)\n",
    "    \n",
    "    # Get frequencies of all nouns and proper nouns in the free-text Series\n",
    "    all_noun_and_propn_freqs = text_spaCy.get_most_common_nouns_and_propns(None, exclude_word_list)\n",
    "    print(str(len(all_noun_and_propn_freqs)), \"unique nouns/proper nouns found\")\n",
    "    \n",
    "    # Get \"synonym\" groupings of the nouns and proper nouns (either wordnet hyponyms or spaCy similarity)\"\n",
    "    top_word_groups = get_top_word_groups_by_synset_then_similarity(\n",
    "        nlp, all_noun_and_propn_freqs, top_n_noun_groups, max_hyponyms, max_hyponym_depth, sim_threshold, None)\n",
    "    \n",
    "    NUM_CONTEXT_WORDS = 4\n",
    "    if top_n_words:\n",
    "        print(\"Top\", top_n_words, \"words\", end='')\n",
    "    else:\n",
    "        print(\"Words\", end='')\n",
    "    print(\" associated with nouns/proper noun groupings, looking at up to\",\n",
    "          str(NUM_CONTEXT_WORDS), \"words before or after each noun:\")\n",
    "\n",
    "    ##print(\"DEBUG: top_word_groups\", top_word_groups)\n",
    "    for item in top_word_groups:\n",
    "        root_word_freq, group_word_freqs = item\n",
    "        root_word, root_word_count = root_word_freq\n",
    "        if root_word != _OTHER_GROUP_ROOT_WORD and root_word != _UNKNOWN_GROUP_ROOT_WORD:\n",
    "            NO_WORD = \"no_word\"\n",
    "            assoc_word_list = [] # List of associated words for each noun/proper nount group\n",
    "            for word_freq in group_word_freqs:\n",
    "                word, count = word_freq\n",
    "                for free_text in free_text_list:\n",
    "                    if word in free_text.lower().rstrip(string.punctuation).split():\n",
    "                        ##print(\"DEBUG:--------------------------------------------------------------------------------\")\n",
    "                        ##print(\"DEBUG: free_text: '\" + free_text + \"'\")\n",
    "                        for before_words, after_words in get_words_around(word, free_text, NUM_CONTEXT_WORDS):\n",
    "                            ##print(\"DEBUG: word(s) before '\" + word + \"': \", before_words)\n",
    "                            ##print(\"DEBUG: word(s) after '\" + word + \"': \", after_words)\n",
    "                            # First try to find an associated word in words before the noun word\n",
    "                            assoc_word = None\n",
    "                            before_words_len = len(before_words)\n",
    "                            for index in range(before_words_len):\n",
    "                                before_word = before_words[before_words_len-index-1]\n",
    "                                if not before_word or before_word in string.punctuation: break\n",
    "                                elif before_word == \"not\" or before_word[-3:] == \"n't\":\n",
    "                                    negation = \"not\"\n",
    "                                    if index != 0 and before_words[before_words_len-index] != word:\n",
    "                                        negation = negation + \"_\" + before_words[before_words_len-index]\n",
    "                                    assoc_word = negation\n",
    "                                    ##print(\"DEBUG: negation '\" + negation + \"' added to assoc_word_list for word '\" +\n",
    "                                    ##      word + \"' in free_text '\" + free_text + \"'\")\n",
    "                                    break\n",
    "                                elif before_word == \"no\":\n",
    "                                    negation = \"no\"\n",
    "                                    if index != 0 and before_words[before_words_len-index] != word:\n",
    "                                        negation = negation + \"_\" + before_words[before_words_len-index]\n",
    "                                    assoc_word = negation\n",
    "                                    ##print(\"DEBUG: negation '\" + negation + \"' added to assoc_word_list for word '\" +\n",
    "                                    ##      word + \"' in free_text '\" + free_text + \"'\")\n",
    "                                    break\n",
    "                                elif before_word not in exclude_word_list:\n",
    "                                    word_before_before_word = before_words[before_words_len-index-2]\n",
    "                                    if word_before_before_word:\n",
    "                                        if word_before_before_word == \"not\" or word_before_before_word[-3:] == \"n't\":\n",
    "                                            negation = \"not_\" + before_word\n",
    "                                            assoc_word = negation\n",
    "                                            ##print(\"DEBUG: negation of before_word '\" + negation + \"' added to assoc_word_list for word '\" +\n",
    "                                            ##      word + \"' in free_text '\" + free_text + \"'\")\n",
    "                                            break\n",
    "                                        else:\n",
    "                                            assoc_word = before_word\n",
    "                                            ##print(\"DEBUG: before_word '\" + before_word + \"' added to assoc_word_list for word '\" +\n",
    "                                            ##      word + \"' in free_text '\" + free_text + \"'\")\n",
    "                                            break\n",
    "                            if not assoc_word:\n",
    "                                # Try to find an associated word following the noun word\n",
    "                                after_words_len = len(after_words)\n",
    "                                for index in range(after_words_len):\n",
    "                                    after_word = after_words[index]\n",
    "                                    if not after_word or after_word in string.punctuation: break\n",
    "                                    elif after_word == \"not\" or after_word[-3:] == \"n't\":\n",
    "                                        if (index+1 < after_words_len):\n",
    "                                            word_after_after_word = after_words[index+1]\n",
    "                                            negation = \"not_\" + word_after_after_word\n",
    "                                            assoc_word = negation\n",
    "                                            ##print(\"DEBUG: negation '\" + negation + \"' added to assoc_word_list for word '\" +\n",
    "                                            ##      word + \"' in free_text '\" + free_text + \"'\")\n",
    "                                        break\n",
    "                                    elif after_word == \"no\":\n",
    "                                        if (index+1 < after_words_len):\n",
    "                                            word_after_after_word = after_words[index+1]\n",
    "                                            negation = \"no_\" + word_after_after_word\n",
    "                                            assoc_word = negation\n",
    "                                            ##print(\"DEBUG: negation '\" + negation + \"' added to assoc_word_list for word '\" +\n",
    "                                            ##      word + \"' in free_text '\" + free_text + \"'\")\n",
    "                                        break\n",
    "                                    elif after_word not in exclude_word_list:\n",
    "                                        assoc_word_list.append(after_word)\n",
    "                                        assoc_word = after_word\n",
    "                                        ##print(\"DEBUG: after_word '\" + after_word + \"' added to assoc_word_list for word '\" +\n",
    "                                        ##      word + \"' in free_text '\" + free_text + \"'\")\n",
    "                                        break\n",
    "                        if not assoc_word:\n",
    "                            # Assume the word itself is the only context for the text - see Notes above\n",
    "                            assoc_word = word\n",
    "                            ##if word == root_word:\n",
    "                            ##    print(\"DEBUG: Topic identified: \" + root_word + \" for '\" + free_text + \"'\")\n",
    "                            ##else:\n",
    "                            ##    print(\"DEBUG: Topic identified: \" + root_word + \": \" + word + \" for '\" + \n",
    "                            ##          free_text + \"'\")\n",
    "                        ##else:\n",
    "                        ##    print(\"DEBUG: Topic identified: \" + root_word + \": \" + assoc_word + \"_\" + word\n",
    "                        ##          + \" for '\" + free_text + \"'\")\n",
    "                        assoc_word_list.append(assoc_word)\n",
    "                        \n",
    "            most_common_words = Counter(assoc_word_list).most_common(top_n_words)\n",
    "            ##print(\"DEBUG: assoc_word_list:\", assoc_word_list)\n",
    "            print(\"'\" + root_word + \"', \" + str(root_word_count) + \": \", end='')\n",
    "            print(most_common_words, end='')\n",
    "            print(\"    {\", end='')\n",
    "            for word_freq in group_word_freqs:\n",
    "                word, count = word_freq\n",
    "                print(\"('\" + word + \"', \" + str(count) + \"), \", end='')\n",
    "            print(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_words_associated_with_common_noun_groups(\n",
    "    nlp, \"**Sample** Yale survey emotion reasons (FEELATWORK_TASKS_<n>_WHY_clean's)\",\n",
    "    sample_reasons['sample_reasons_concat'], exclude_words, 10, None, 100, 1, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_words_associated_with_common_noun_groups(\n",
    "    nlp, \"Yale survey emotion reasons (FEELATWORK_TASKS_<n>_WHY_clean's)\", all_reasons['all_reasons_concat'],\n",
    "    exclude_words, 300, 10, 100, 1, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
